{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c70d56",
   "metadata": {},
   "source": [
    "# TD Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a86dc",
   "metadata": {},
   "source": [
    "# Tutorial - Deep Q-Learning \n",
    "\n",
    "Deep Q-Learning uses a neural network to approximate $Q$ functions. Hence, we usually refer to this algorithm as DQN (for *deep Q network*).\n",
    "\n",
    "The parameters of the neural network are denoted by $\\theta$. \n",
    "*   As input, the network takes a state $s$,\n",
    "*   As output, the network returns $Q_\\theta [a | s] = Q_\\theta (s,a) = Q(s, a, \\theta)$, the value of each action $a$ in state $s$, according to the parameters $\\theta$.\n",
    "\n",
    "\n",
    "The goal of Deep Q-Learning is to learn the parameters $\\theta$ so that $Q(s, a, \\theta)$ approximates well the optimal $Q$-function $Q^*(s, a) \\simeq Q_{\\theta^*} (s,a)$. \n",
    "\n",
    "In addition to the network with parameters $\\theta$, the algorithm keeps another network with the same architecture and parameters $\\theta^-$, called **target network**.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1.   At each time $t$, the agent is in state $s_t$ and has observed the transitions $(s_i, a_i, r_i, s_i')_{i=1}^{t-1}$, which are stored in a **replay buffer**.\n",
    "\n",
    "2.  Choose action $a_t = \\arg\\max_a Q_\\theta(s_t, a)$ with probability $1-\\varepsilon_t$, and $a_t$=random action with probability $\\varepsilon_t$. \n",
    "\n",
    "3. Take action $a_t$, observe reward $r_t$ and next state $s_t'$.\n",
    "\n",
    "4. Add transition $(s_t, a_t, r_t, s_t')$ to the **replay buffer**.\n",
    "\n",
    "4.  Sample a minibatch $\\mathcal{B}$ containing $B$ transitions from the replay buffer. Using this minibatch, we define the loss:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\sum_{(s_i, a_i, r_i, s_i') \\in \\mathcal{B}}\n",
    "\\left[\n",
    "Q(s_i, a_i, \\theta) -  y_i\n",
    "\\right]^2\n",
    "$$\n",
    "where the $y_i$ are the **targets** computed with the **target network** $\\theta^-$:\n",
    "\n",
    "$$\n",
    "y_i = r_i + \\gamma \\max_{a'} Q(s_i', a', \\theta^-).\n",
    "$$\n",
    "\n",
    "5. Update the parameters $\\theta$ to minimize the loss, e.g., with gradient descent (**keeping $\\theta^-$ fixed**): \n",
    "$$\n",
    "\\theta \\gets \\theta - \\eta \\nabla_\\theta L(\\theta)\n",
    "$$\n",
    "where $\\eta$ is the optimization learning rate. \n",
    "\n",
    "6. Every $N$ transitions ($t\\mod N$ = 0), update target parameters: $\\theta^- \\gets \\theta$.\n",
    "\n",
    "7. $t \\gets t+1$. Stop if $t = T$, otherwise go to step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3192463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "# from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea50e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python --version = 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:23:11) \n",
      "[GCC 9.4.0]\n",
      "torch.__version__ = 1.10.1+cu113\n",
      "np.__version__ = 1.21.4\n",
      "gym.__version__ = 0.21.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"python --version = {sys.version}\")\n",
    "print(f\"torch.__version__ = {torch.__version__}\")\n",
    "print(f\"np.__version__ = {np.__version__}\")\n",
    "print(f\"gym.__version__ = {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117afb1",
   "metadata": {},
   "source": [
    "## Torch 101\n",
    "\n",
    ">\"The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities. \n",
    "[...] provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.\" \n",
    "[PyTorch](https://pytorch.org/docs/stable/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f94aaf",
   "metadata": {},
   "source": [
    "### Variable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea2eaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_torch is of type <class 'torch.Tensor'>\n",
      "\n",
      "Float:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "Int:\n",
      " tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]])\n",
      "Bool:\n",
      " tensor([[False, False],\n",
      "        [False, False],\n",
      "        [False, False]])\n",
      "\n",
      "View new shape... tensor([[0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "Algebraic operations are overloaded:\n",
      " tensor([[-0.3970,  0.0537],\n",
      "        [-1.0574,  0.3664],\n",
      "        [ 1.2580,  0.6596]]) \n",
      "+\n",
      " tensor([[ 1.1747,  0.5316],\n",
      "        [ 1.2962, -1.3220],\n",
      "        [ 0.6024,  2.3224]]) \n",
      "=\n",
      " tensor([[ 0.7777,  0.5853],\n",
      "        [ 0.2388, -0.9556],\n",
      "        [ 1.8604,  2.9820]])\n"
     ]
    }
   ],
   "source": [
    "# Very similar syntax to numpy.\n",
    "zero_torch = torch.zeros((3, 2))\n",
    "\n",
    "print('zero_torch is of type {:s}'.format(str(type(zero_torch))))\n",
    "\n",
    "# Torch -> Numpy: simply call the numpy() method.\n",
    "zero_np = np.zeros((3, 2))\n",
    "assert (zero_torch.numpy() == zero_np).all()\n",
    "\n",
    "# Numpy -> Torch: simply call the corresponding function on the np.array.\n",
    "zero_torch_float = torch.FloatTensor(zero_np)\n",
    "print('\\nFloat:\\n', zero_torch_float)\n",
    "zero_torch_int = torch.LongTensor(zero_np)\n",
    "print('Int:\\n', zero_torch_int)\n",
    "zero_torch_bool = torch.BoolTensor(zero_np)\n",
    "print('Bool:\\n', zero_torch_bool)\n",
    "\n",
    "# Reshape\n",
    "print('\\nView new shape...', zero_torch.view(1, 6))\n",
    "# Note that print(zero_torch.reshape(1, 6)) would work too.\n",
    "# The difference is in how memory is handled (view imposes contiguity).\n",
    "\n",
    "# Algebra\n",
    "a = torch.randn((3, 2))\n",
    "b = torch.randn((3, 2))\n",
    "print('\\nAlgebraic operations are overloaded:\\n', a, '\\n+\\n', b, '\\n=\\n', a+b )\n",
    "\n",
    "# More generally, torch shares the syntax of many attributes and functions with Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7f1191",
   "metadata": {},
   "source": [
    "### Gradient management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a2a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "Initial guess: tensor([1.3004])\n",
      "Final estimate: tensor([2.0020])\n",
      "The final estimate should be close to tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor is a similar yet more complicated data structure than np.array.\n",
    "# It is basically a static array of number but may also contain an overlay to \n",
    "# handle automatic differentiation (i.e keeping track of the gradient and which \n",
    "# tensors depend on which).\n",
    "# To access the static array embedded in a tensor, simply call the detach() method\n",
    "print(zero_torch.detach())\n",
    "\n",
    "# When inside a function performing automatic differentiation (basically when training \n",
    "# a neural network), never use detach() otherwise meta information regarding gradients\n",
    "# will be lost, effectively freezing the variable and preventing backprop for it. \n",
    "# However when returning the result of training, do use detach() to save memory \n",
    "# (the naked tensor data uses much less memory than the full-blown tensor with gradient\n",
    "# management, and is much less prone to mistake such as bad copy and memory leak).\n",
    "\n",
    "# We will solve theta * x = y in theta for x=1 and y=2\n",
    "x = torch.ones(1)\n",
    "y = 2 * torch.ones(1)\n",
    "\n",
    "# Actually by default torch does not add the gradient management overlay\n",
    "# when declaring tensors like this. To force it, add requires_grad=True.\n",
    "theta = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Optimisation routine\n",
    "# (Adam is a sophisticated variant of SGD, with adaptive step).\n",
    "optimizer = optim.Adam(params=[theta], lr=0.1)\n",
    "\n",
    "# Loss function\n",
    "print('Initial guess:', theta.detach())\n",
    "\n",
    "for _ in range(100):\n",
    "    # By default, torch accumulates gradients in memory.\n",
    "    # To obtain the desired gradient descent beahviour,\n",
    "    # just clean the cached gradients using the following line:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Quadratic loss (* and ** are overloaded so that torch\n",
    "    # knows how to differentiate them)\n",
    "    loss = (y - theta * x) ** 2\n",
    "    \n",
    "    # Apply the chain rule to automatically compute gradients\n",
    "    # for all relevant tensors.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Run one step of optimisation routine.\n",
    "    optimizer.step()\n",
    "    \n",
    "print('Final estimate:', theta.detach())\n",
    "print('The final estimate should be close to', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f014ed38",
   "metadata": {},
   "source": [
    "## Setting the environment\n",
    "\n",
    "### 1 - Define the GLOBAL parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03a05cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Discount factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 256\n",
    "# Capacity of the replay buffer\n",
    "BUFFER_CAPACITY = 16384 # 10000\n",
    "# Update target net every ... episodes\n",
    "UPDATE_TARGET_EVERY = 32 # 20\n",
    "\n",
    "# Initial value of epsilon\n",
    "EPSILON_START = 1.0\n",
    "# Parameter to decrease epsilon\n",
    "DECREASE_EPSILON = 500\n",
    "# Minimum value of epislon\n",
    "EPSILON_MIN = 0.05\n",
    "\n",
    "# Number of training episodes\n",
    "# N_EPISODES = 200 #Default\n",
    "N_EPISODES = 500\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "#Added by me\n",
    "# SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38d6dd",
   "metadata": {},
   "source": [
    "### 2 - Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbd61940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = (state, action, reward, next_state)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.choices(self.memory, k=batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# create instance of replay buffer\n",
    "replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64a096",
   "metadata": {},
   "source": [
    "### 3 - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0decdfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic neural net.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eaf259",
   "metadata": {},
   "source": [
    "### 3.5 - Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4858d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network and target network\n",
    "hidden_size = 128\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# torch.manual_seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# random.seed(SEED)\n",
    "q_net = Net(obs_size, hidden_size, n_actions)\n",
    "target_net = Net(obs_size, hidden_size, n_actions)\n",
    "\n",
    "# objective and optimizer\n",
    "objective = nn.MSELoss()\n",
    "optimizer = optim.Adam(params=q_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7975fdd2",
   "metadata": {},
   "source": [
    "### Question 0 (to do at home, not during the live session)\n",
    "\n",
    "With your own word, explain the intuition behind DQN. Recall the main parts of the aformentionned algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b72807",
   "metadata": {},
   "source": [
    "## Implementing the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "652a3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q(states):\n",
    "    \"\"\"\n",
    "    Compute Q function for a list of states\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        states_v = torch.FloatTensor([states])\n",
    "        output = q_net.forward(states_v).detach().numpy()  # shape (1, len(states), n_actions)\n",
    "    return output[0, :, :]  # shape (len(states), n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb2b2c",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "Implement the `choose_action` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1672f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    \"\"\"\n",
    "    ** TO BE IMPLEMENTED **\n",
    "    \n",
    "    Return action according to an epsilon-greedy exploration policy\n",
    "    \"\"\"\n",
    "    if np.random.uniform() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else :\n",
    "        return np.argmax(get_q([state])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3302b69f",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Implement the `eval_dqn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e15cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dqn(n_sim=5):\n",
    "    \"\"\"\n",
    "    ** TO BE IMPLEMENTED **\n",
    "    \n",
    "    Monte Carlo evaluation of DQN agent.\n",
    "\n",
    "    Repeat n_sim times:\n",
    "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
    "        * Compute the sum of rewards in this episode\n",
    "        * Store the sum of rewards in the episode_rewards array.\n",
    "    \"\"\"\n",
    "    env_copy = deepcopy(env)\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "    for i in range(n_sim):\n",
    "        state = env_copy.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = choose_action(state, 0.0)\n",
    "            state, reward, done, _ = env_copy.step(action)\n",
    "            episode_rewards[i] += reward\n",
    "    \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f2b0a",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "Implement the `update` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d1e6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    ** TO BE COMPLETED **\n",
    "    \"\"\"\n",
    "    \n",
    "    # add data to replay buffer\n",
    "    if done:\n",
    "        next_state = None\n",
    "    replay_buffer.push(state, action, reward, next_state)\n",
    "    \n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        return np.inf\n",
    "    \n",
    "    # get batch\n",
    "    transitions = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "    # Compute loss - TO BE IMPLEMENTED!\n",
    "    # ---------------------------------------------------------------------------- #\n",
    "    #                                  Build info                                  #\n",
    "    # ---------------------------------------------------------------------------- #\n",
    "    old_states = torch.FloatTensor([t[0] for t in transitions])\n",
    "    actions = torch.LongTensor([t[1] for t in transitions]).view(-1, 1)\n",
    "    rewards = torch.FloatTensor([t[2] for t in transitions]).view(-1, 1)\n",
    "    it_is_done = torch.BoolTensor([type(t[3]) != np.ndarray for t in transitions]).view(-1)\n",
    "    # --------------------------------- fill None -------------------------------- #\n",
    "    next_states_preprocess = [t[3] for t in transitions]\n",
    "    for i in range(len(next_states_preprocess)):\n",
    "        if type(next_states_preprocess[i]) != np.ndarray:\n",
    "            next_states_preprocess[i] = torch.zeros(obs_size).numpy()\n",
    "    next_states = torch.FloatTensor(next_states_preprocess)\n",
    "    # ---------------------------------------------------------------------------- #\n",
    "    #                                    Forward                                   #\n",
    "    # ---------------------------------------------------------------------------- #\n",
    "    # --------------------------------- q network -------------------------------- #\n",
    "    values  = q_net(old_states).gather(dim = 1 , index=actions).view(-1, 1) # take the logit of the taken action\n",
    "    # ------------------------------ target network ------------------------------ #\n",
    "    targets = target_net(next_states).max(dim=1)[0].detach().view(-1, 1)\n",
    "    targets[it_is_done] = 0.0\n",
    "    targets = GAMMA * targets + rewards\n",
    "    # ----------------------------------- loss ----------------------------------- #\n",
    "    loss = objective(values, targets)\n",
    "\n",
    "    # Optimize the model - UNCOMMENT!\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0152c68",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Train a DQN on the `env` environment.\n",
    "*Hint* The mean reward after training should be close to 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a59e924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 5 , reward =  9.6\n",
      "episode = 10 , reward =  8.4\n",
      "episode = 15 , reward =  9.6\n",
      "episode = 20 , reward =  25.4\n",
      "episode = 25 , reward =  17.2\n",
      "episode = 30 , reward =  22.4\n",
      "episode = 35 , reward =  11.0\n",
      "episode = 40 , reward =  9.0\n",
      "episode = 45 , reward =  32.4\n",
      "episode = 50 , reward =  10.6\n",
      "episode = 55 , reward =  9.4\n",
      "episode = 60 , reward =  9.8\n",
      "episode = 65 , reward =  12.8\n",
      "episode = 70 , reward =  9.4\n",
      "episode = 75 , reward =  9.4\n",
      "episode = 80 , reward =  18.4\n",
      "episode = 85 , reward =  9.0\n",
      "episode = 90 , reward =  9.8\n",
      "episode = 95 , reward =  9.2\n",
      "episode = 100 , reward =  10.2\n",
      "episode = 105 , reward =  56.2\n",
      "episode = 110 , reward =  9.6\n",
      "episode = 115 , reward =  10.2\n",
      "episode = 120 , reward =  10.4\n",
      "episode = 125 , reward =  9.4\n",
      "episode = 130 , reward =  107.2\n",
      "episode = 135 , reward =  10.4\n",
      "episode = 140 , reward =  11.2\n",
      "episode = 145 , reward =  10.6\n",
      "episode = 150 , reward =  10.2\n",
      "episode = 155 , reward =  10.8\n",
      "episode = 160 , reward =  24.8\n",
      "episode = 165 , reward =  40.2\n",
      "episode = 170 , reward =  38.0\n",
      "episode = 175 , reward =  32.2\n",
      "episode = 180 , reward =  67.6\n",
      "episode = 185 , reward =  57.6\n",
      "episode = 190 , reward =  44.6\n",
      "episode = 195 , reward =  60.2\n",
      "episode = 200 , reward =  44.4\n",
      "episode = 205 , reward =  71.8\n",
      "episode = 210 , reward =  11.2\n",
      "episode = 215 , reward =  40.4\n",
      "episode = 220 , reward =  58.6\n",
      "episode = 225 , reward =  36.4\n",
      "episode = 230 , reward =  87.6\n",
      "episode = 235 , reward =  75.6\n",
      "episode = 240 , reward =  54.6\n",
      "episode = 245 , reward =  82.0\n",
      "episode = 250 , reward =  68.0\n",
      "episode = 255 , reward =  54.6\n",
      "episode = 260 , reward =  56.6\n",
      "episode = 265 , reward =  200.0\n",
      "\n",
      "mean reward after training =  200.0\n"
     ]
    }
   ],
   "source": [
    "EVAL_EVERY = 5\n",
    "REWARD_THRESHOLD = 199\n",
    "\n",
    "def train():\n",
    "    state = env.reset()\n",
    "    epsilon = EPSILON_START\n",
    "    ep = 0\n",
    "    total_time = 0\n",
    "    while ep < N_EPISODES:\n",
    "        action = choose_action(state, epsilon)\n",
    "\n",
    "        # take action and update replay buffer and networks\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        loss = update(state, action, reward, next_state, done)\n",
    "\n",
    "        # update state\n",
    "        state = next_state\n",
    "\n",
    "        # end episode if done\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            ep   += 1\n",
    "            if ( (ep+1)% EVAL_EVERY == 0):\n",
    "                rewards = eval_dqn()\n",
    "                print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n",
    "                if np.mean(rewards) >= REWARD_THRESHOLD:\n",
    "                    break\n",
    "\n",
    "            # update target network\n",
    "            if ep % UPDATE_TARGET_EVERY == 0:\n",
    "                target_net.load_state_dict(q_net.state_dict())\n",
    "            # decrease epsilon\n",
    "            epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN) * \\\n",
    "                            np.exp(-1. * ep / DECREASE_EPSILON )    \n",
    "\n",
    "        total_time += 1\n",
    "\n",
    "# Run the training loop\n",
    "train()\n",
    "\n",
    "# Evaluate the final policy\n",
    "rewards = eval_dqn(20)\n",
    "print(\"\")\n",
    "print(\"mean reward after training = \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90708d1",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "Experiment the policy network.\n",
    "\n",
    "(Showing a video with a jupyter notebook, you may try this cell with Chrome/Chromium instead of Firefox. Otherwise, you may skip this question.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1680e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video():\n",
    "    html = []\n",
    "    for mp4 in Path(\"videos\").glob(\"*.mp4\"):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append('''<video alt=\"{}\" autoplay \n",
    "                      loop controls style=\"height: 400px;\">\n",
    "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "    \n",
    "env = Monitor(env, './videos', force=True, video_callable=lambda episode: True)\n",
    "\n",
    "for episode in range(2):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = choose_action(state, 0.0)\n",
    "        state, reward, done, info = env.step(action)\n",
    "# env.close()\n",
    "# show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb94bc",
   "metadata": {},
   "source": [
    "### Experiments: Do It Yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae9fdf",
   "metadata": {},
   "source": [
    "Remember the set of global parameters:\n",
    "```\n",
    "# Environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Discount factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 256\n",
    "# Capacity of the replay buffer\n",
    "BUFFER_CAPACITY = 16384 # 10000\n",
    "# Update target net every ... episodes\n",
    "UPDATE_TARGET_EVERY = 32 # 20\n",
    "\n",
    "# Initial value of epsilon\n",
    "EPSILON_START = 1.0\n",
    "# Parameter to decrease epsilon\n",
    "DECREASE_EPSILON = 200\n",
    "# Minimum value of epislon\n",
    "EPSILON_MIN = 0.05\n",
    "\n",
    "# Number of training episodes\n",
    "N_EPISODES = 200\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba2f8e",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "\n",
    "Craft an experiment and study the influence of the `BUFFER_CAPACITY` on the learning process (speed of *convergence*, training curves...) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1533f",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "\n",
    "Craft an experiment and study the influence of the `UPDATE_TARGET_EVERY` on the learning process (speed of *convergence*, training curves...) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f197f12",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "\n",
    "If you have the computer power to do so, try to do a grid search on those two hyper-parameters and comment the results. Otherwise, study the influence of another hyper-parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc76ffb",
   "metadata": {},
   "source": [
    "## Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b35d83",
   "metadata": {},
   "source": [
    "It is natural to use a function approximator like a neural network to approximate the $Q$ function in a continuous environment. Another natural but unscalable way to do handle continuous state-action space is **discretization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1525f",
   "metadata": {},
   "source": [
    "Discretize the environment of your choice (cartpole or mountain car or both) and run one of the algorithms that you know to compute an approximation of the optimal $Q$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c32ad0",
   "metadata": {},
   "source": [
    "Once you are satisfied with your results, you may plot the *optimal phase diagram* of the system. For instance, you may get something like this for the mountain car environment.\n",
    "![Phase diagram](./phase_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Everything! This is an introduction to research and development"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
