{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML('<style>.container { width:95% !important; } </style>'))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session - Planning in bandits/MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from tictactoe_env import TicTacToeEnv, DRAW\n",
    "from utils import play, Agent, agent_vs_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I -  Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical session, we will implement several MCTS heuristics and compare them. To do so, we will use a  Tic Tac Toe environement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic Tac Toe environment\n",
    "\n",
    "TicTacToeEnv is a custom envirionment based on the OpenAI gym library.\n",
    "\n",
    "Disclaimer: this implementation is *not* optimal and rather slow (due to copying heavy gym objects in many places). The goal here is to be pedagogical, not time-efficient. Don't think MCTS is necessarily slow in general!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToeEnv()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example game: each player plays randomly\n",
    "\n",
    "# Always reset the environment at the start of a game\n",
    "# (Remark : the reset method returns the initial state \n",
    "# of the envionment, i.e the board in the present case)\n",
    "env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    # Draw a position uniformly at random among the\n",
    "    # remaining possible choices\n",
    "    pos = env.sample()\n",
    "    board, reward, done, _ = env.step(pos)\n",
    "    # Display the board\n",
    "    env.render()\n",
    "\n",
    "winner = board.check_state()\n",
    "if winner == DRAW:\n",
    "    print(\"**      DRAW      **\")\n",
    "elif winner == 1:\n",
    "    print(\"**      O WINS      **\")\n",
    "else:\n",
    "    print(\"**      X WINS      **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a dummy agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyAgent(Agent):\n",
    "    \"\"\"Dummy TicTacToe agent that plays randomly.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, player=1, params={}):\n",
    "        self._env = env\n",
    "        self.player = player  # index of the player\n",
    "\n",
    "    def play(self):\n",
    "        \"\"\"Syntax: play should be a method with no argument\n",
    "        except self.\n",
    "        In the present example, it simply calls the env attribute\n",
    "        and use it to sample a feasible action.\n",
    "        \"\"\"\n",
    "        return self.env.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available input in the API: [1-9], 'quit', 'switch'\n",
    "play(DummyAgent, game_env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitting two agents against one another "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_vs_agent(env, DummyAgent, DummyAgent, n_episodes=1000, params1={}, params2={}, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Comment the results of random vs random above. How do you explain that 'O' has the advantage despite playing at random?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Recall what Q-learning is.\n",
    "\n",
    "In **qlearning.py**, the Q-learning update rule is missing from the functions Q_learning_vs_random and random_vs_Q_learning. Complete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlearning import QFunction, QLearningAgent, random_vs_Q_learning, Q_learning_vs_random\n",
    "\n",
    "GAMMA = 0.99\n",
    "n_episodes = int(1e4)\n",
    "learning_rate = 0.05\n",
    "epsilon_greedy = 0.9\n",
    "epsilon_greedy_decay_freq = 50\n",
    "epsilon_greedy_decay_factor = 0.9\n",
    "\n",
    "Q_opt_Q_learning_cross, stats = random_vs_Q_learning(\n",
    "    env,\n",
    "    n_episodes,\n",
    "    learning_rate,\n",
    "    epsilon_greedy,\n",
    "    epsilon_greedy_decay_freq,\n",
    "    epsilon_greedy_decay_factor,\n",
    "    gamma=GAMMA,\n",
    ")\n",
    "\n",
    "Q_opt_Q_learning_nought, stats = Q_learning_vs_random(\n",
    "    env,\n",
    "    n_episodes,\n",
    "    learning_rate,\n",
    "    epsilon_greedy,\n",
    "    epsilon_greedy_decay_freq,\n",
    "    epsilon_greedy_decay_factor,\n",
    "    gamma=GAMMA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(QLearningAgent, game_env=env, params={'Q': Q_opt_Q_learning_cross.Q})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Do you think Q-Learning is performing well on this game? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Recall and explain the idea behind MCTS and the four main steps of a MCTS.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A template class for MCTS is defined in **mcts.py**. It is an abstract class, and you need to instanciate the method search_heuristic, to guide the selection step of MCTS.\n",
    "\n",
    "We first implement the uniform random search.\n",
    "\n",
    "### Question 5\n",
    "\n",
    "- Understand the pure exploration tree search heuristic implementation\n",
    "- Implement the UCT algorithm\n",
    "\n",
    "Feel free to experiment and implement other tree search policies (e.g., implement a pure exploitation tree search heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcts import MCTS\n",
    "\n",
    "class MCTSPureExploration(MCTS):\n",
    "    def __init__(self, player=1):\n",
    "        super().__init__(player=player)\n",
    "\n",
    "    def search_heuristic(self, node):\n",
    "        \"\"\"Pure exploration, uniformly choose how to\n",
    "        explore the game tree.\n",
    "        \"\"\"\n",
    "        # All children of node should already be expanded:\n",
    "        assert all(n in self.children for n in self.children[node])\n",
    "        \n",
    "        return np.random.choice(list(self.children[node]))\n",
    "\n",
    "\n",
    "class MCTSUCT(MCTS):\n",
    "    def __init__(self, player=1, exploration_param=1.0):\n",
    "        self.exploration_param = exploration_param\n",
    "        super().__init__(player=player)\n",
    "\n",
    "    def search_heuristic(self, node):\n",
    "        \"\"\"Upper Confidence bound for Trees.\n",
    "        Balance exploration and exploitation in the game tree.\n",
    "        \"\"\"\n",
    "        # All children of node should already be expanded:\n",
    "        assert all(n in self.children for n in self.children[node])\n",
    "        \n",
    "        # children is a dictionary of node -> set of children,\n",
    "        # where node is a TicTacToeEnv, and the chidlren are\n",
    "        # TicTacToeEnv corresponding to boards that are \n",
    "        # one action away from node.\n",
    "        \n",
    "        # self.W is a dictionary of node -> backpropagated rewards\n",
    "        # self.N is a dictionary of node -> backpropagated number of visits\n",
    "        \n",
    "        def uct(n):\n",
    "            \"Upper confidence bound for trees\"\n",
    "            # TODO\n",
    "        return max(self.children[node], key=uct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSAgent(Agent):\n",
    "    \"\"\"TicTacToe template agent that plays according to MCTS.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, player=1, params={'n_playouts': 1}):\n",
    "        self.env = env\n",
    "        self.player = player  # index of the player\n",
    "        self.n_playouts = params.get('n_playouts', 1)\n",
    "        # To be implemented in child class\n",
    "        self.game_tree = None \n",
    "        \n",
    "    def play(self):\n",
    "        \"\"\"Syntax: play should be a method without argument\n",
    "        except self.\n",
    "        \"\"\"\n",
    "        # Need to copy environment, so that MCTS internal simulations\n",
    "        # do not affect the game being played\n",
    "        env_copy = deepcopy(env)\n",
    "        for _ in range(self.n_playouts):\n",
    "            self.game_tree.playout(env_copy)\n",
    "        return self.game_tree.choose(env_copy)\n",
    "\n",
    "\n",
    "class MCTSPureExplorationAgent(MCTSAgent):\n",
    "    \"\"\"TicTacToe agent that plays according to MCTS Pure Exploration.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, player=1, params={'n_playouts': 1}):\n",
    "        super().__init__(env=env, player=player, params=params)\n",
    "        self.game_tree = MCTSPureExploration(player=player)\n",
    "\n",
    "        \n",
    "class MCTSUCTAgent(MCTSAgent):\n",
    "    \"\"\"TicTacToe agent that plays according to MCTS UCT.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, player=1, params={'n_playouts': 1, 'exploration_param': 1.0}):\n",
    "        super().__init__(env=env, player=player, params=params)\n",
    "        exploration_param = params.get('exploration_param', 1.0)\n",
    "        self.game_tree = MCTSUCT(player=player, exploration_param=exploration_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "agent1 = DummyAgent(env, player=1)\n",
    "agent2 = MCTSUCTAgent(env, player=2, params={'n_playouts': 20, 'exploration_param': 1.0})\n",
    "\n",
    "n_episodes = 10\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    done = False\n",
    "    _ = env.reset()\n",
    "    turn = 0\n",
    "\n",
    "    while not done:\n",
    "        if turn % 2 == 0:\n",
    "            action = agent1.play()\n",
    "        else:\n",
    "            action = agent2.play()\n",
    "\n",
    "        _, _, done, _ = env.step(action)\n",
    "\n",
    "        turn += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    winner = env.board.check_state()\n",
    "    if winner == DRAW:\n",
    "        print(\"**      DRAW        **\")\n",
    "    elif winner == 1:\n",
    "        print(\"**      O WINS      **\")\n",
    "    else:\n",
    "        print(\"**      X WINS      **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Interpret the output of the next cell. What does it represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_tree = agent2.game_tree\n",
    "children = game_tree.children\n",
    "print('Total number of nodes: {}\\n'.format(len(children)))\n",
    "for node, successors in children.items():\n",
    "    print('Node: {} wins, {} visits'.format(game_tree.W[node], game_tree.N[node]))\n",
    "    node.render()\n",
    "    for successor in successors:\n",
    "        print('Child: {} wins, {} visits'.format(game_tree.W[successor], game_tree.N[successor]))\n",
    "        successor.render()\n",
    "    print('----------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Run the following cells and play against the MCTS AI.\n",
    "\n",
    "Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_vs_agent(env, DummyAgent, MCTSPureExplorationAgent, n_episodes=10, params1={}, params2={'n_playouts': 20}, plot=True)\n",
    "agent_vs_agent(env, DummyAgent, MCTSUCTAgent, n_episodes=10, params1={}, params2={'n_playouts': 20}, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(MCTSUCTAgent, game_env=env, params={'n_playouts': 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_vs_agent(env, MCTSUCTAgent, QLearningAgent, n_episodes=10, params1={'n_playouts': 50}, params2={'Q': Q_opt_Q_learning_cross.Q}, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
